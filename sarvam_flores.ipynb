{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8545a548",
   "metadata": {},
   "source": [
    "## Inference Using Sarvam Translate Model on Flores plus dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43689cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "# from evaluate import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d28bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for doi_Deva  not in Flores Plus\n",
    "# In_langs = ['asm_Beng','ben_Beng','brx_Deva','doi_Deva','eng_Latn','gom_Deva','guj_Gujr','hin_Deva','kan_Knda','kas_Arab','kas_Deva','mai_Deva','mal_Mlym','mar_Deva','mni_Beng','mni_Mtei','npi_Deva','ory_Orya','pan_Guru','san_Deva','sat_Olck','snd_Arab','snd_Deva','tam_Taml','tel_Telu','urd_Arab']\n",
    "In_langs = ['asm_Beng','ben_Beng','brx_Deva','eng_Latn','gom_Deva','guj_Gujr','hin_Deva','kan_Knda','kas_Arab','kas_Deva','mai_Deva','mal_Mlym','mar_Deva','mni_Beng','mni_Mtei','npi_Deva','ory_Orya','pan_Guru','san_Deva','sat_Olck','snd_Arab','snd_Deva','tam_Taml','tel_Telu','urd_Arab']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7800d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_lang_map = {\n",
    "    'asm_Beng': 'Assamese',\n",
    "    'ben_Beng': 'Bengali',\n",
    "    'brx_Deva': 'Bodo',\n",
    "    'doi_Deva':'Dogri',\n",
    "    'eng_Latn': 'English',\n",
    "    'gom_Deva': 'Konkani',\n",
    "    'guj_Gujr': 'Gujarati',\n",
    "    'hin_Deva': 'Hindi',\n",
    "    'kan_Knda': 'Kannada',\n",
    "    'kas_Arab': 'Kashmiri Arabic Script',\n",
    "    'kas_Deva': 'Kahmiri Devangiri Script',\n",
    "    'mai_Deva': 'Maithili',\n",
    "    'mal_Mlym': 'Malayalam',\n",
    "    'mar_Deva': 'Marathi',\n",
    "    'mni_Beng':'Meiteilon Bengali Script',\n",
    "    'mni_Mtei': 'Meiteilon Script',\n",
    "    'npi_Deva': 'Nepali',\n",
    "    'ory_Orya': 'Odia',\n",
    "    'pan_Guru': 'Punjabi',\n",
    "    'san_Deva': 'Sanskrit',\n",
    "    'sat_Olck': 'Santali',\n",
    "    'snd_Arab' : 'Sindhi Arabic Script',\n",
    "    'snd_Deva': 'Sindhi Devangari Script',\n",
    "    'tam_Taml': 'Tamil',\n",
    "    'tel_Telu': 'Telugu',\n",
    "    'urd_Arab': 'Urdu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95171817",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code_map = {\n",
    "    'Assamese': 'asm_Beng',\n",
    "    'Bengali': 'ben_Beng',\n",
    "    'Bodo': 'brx_Deva',\n",
    "    'Dogri': 'doi_Deva',\n",
    "    'English': 'eng_Latn',\n",
    "    'Konkani': 'gom_Deva',\n",
    "    'Gujarati': 'guj_Gujr',\n",
    "    'Hindi': 'hin_Deva',\n",
    "    'Kannada': 'kan_Knda',\n",
    "    'Kashmiri Arabic Script': 'kas_Arab',\n",
    "    'Kahmiri Devangiri Script': 'kas_Deva',\n",
    "    'Maithili': 'mai_Deva',\n",
    "    'Malayalam': 'mal_Mlym',\n",
    "    'Marathi': 'mar_Deva',\n",
    "    'Meiteilon Bengali Script':'mni_Beng',\n",
    "    'Meiteilon Script': 'mni_Mtei',\n",
    "    'Nepali': 'npi_Deva',\n",
    "    'Odia': 'ory_Orya',\n",
    "    'Punjabi': 'pan_Guru',\n",
    "    'Sanskrit': 'san_Deva',\n",
    "    'Santali': 'sat_Olck',\n",
    "    'Sindhi Devangari Script': 'snd_Deva',\n",
    "    'Sindhi Arabic Script': 'snd_Arab',\n",
    "    'Tamil': 'tam_Taml',\n",
    "    'Telugu': 'tel_Telu',\n",
    "    'Urdu': 'urd_Arab'\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b43aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da66ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lan in In_langs:\n",
    "    ds[lan] = load_dataset(\"openlanguagedata/flores_plus\",lan,split='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ab5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['eng_Latn']['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fcc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sarvamai/sarvam-translate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad13b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the translations will be saved in this dict\n",
    "results={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b12469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing results if available\n",
    "try:\n",
    "    with open(\"translations_sarvam_flores.json\", \"r\",encoding=\"utf-8\") as f:\n",
    "        results = json.load(f)\n",
    "except:\n",
    "    print(\"File does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54726bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentences, src_lang,tgt_lang, batch_size=20):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    print(\"**\"*20, end=\" \")\n",
    "    print(f\"Translating from {src_lang} to {tgt_lang}\",end=\" \")\n",
    "    print(\"**\"*20)\n",
    "    with torch.inference_mode():\n",
    "        for i in range(0, len(sentences), batch_size):\n",
    "            batch = sentences[i:i+batch_size]\n",
    "\n",
    "            # Build messages for each sentence in the batch\n",
    "            messages = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": f\"Translate the text below from {src_lang} to {tgt_lang}.\"},\n",
    "                    {\"role\": \"user\", \"content\": sent}\n",
    "                ]\n",
    "                for sent in batch\n",
    "            ]\n",
    "\n",
    "            # Apply chat template to each\n",
    "            inputs = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]\n",
    "\n",
    "            # Tokenize and move to device\n",
    "            model_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=\"max_length\", max_length=1024, truncation=True).to(model.device)\n",
    "\n",
    "            # Generate\n",
    "            generated_ids = model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=False,\n",
    "                # temperature=0.01,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "            # Decode only new tokens for each sentence\n",
    "            for j, gen in enumerate(generated_ids):\n",
    "                output_ids = gen[len(model_inputs.input_ids[j]):].tolist()\n",
    "                output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "                translations.append(output_text.strip())\n",
    "            \n",
    "            print(f\"Processed {src_lang}-{tgt_lang} {i + len(batch)}/{len(sentences)} sentences\", end='\\r')\n",
    "            \n",
    "            # Clean up to save memory\n",
    "            del model_inputs, generated_ids\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92938304",
   "metadata": {},
   "source": [
    "Indian to Indain languages translation is not supported by this model. Indian to English and English to Indian is alone possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d055b",
   "metadata": {},
   "source": [
    "## English to Indian Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_lang in ['eng_Latn']:\n",
    "    if src_lang not in results:\n",
    "        results[src_lang] = {}\n",
    "    for tgt_lang in In_langs:\n",
    "        if results[src_lang].get(f'{src_lang}-{tgt_lang}'):\n",
    "            print(f\"Skipping {src_lang} to {tgt_lang}, already exists.\")\n",
    "            continue\n",
    "        out = translate(ds[src_lang]['text'], code_lang_map[src_lang], code_lang_map[tgt_lang])\n",
    "        results[src_lang][f'{src_lang}-{tgt_lang}'] = out\n",
    "\n",
    "        # One every translation pair, save to a file\n",
    "\n",
    "        try:\n",
    "            with open(\"translations_sarvam_flores.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "                print(f\"File saved as of {tgt_lang}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114f6f8",
   "metadata": {},
   "source": [
    "## Indian Languages to English Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_lang in In_langs:\n",
    "    if src_lang not in results:\n",
    "        results[src_lang] = {}\n",
    "    for tgt_lang in ['eng_Latn']:\n",
    "        if results[src_lang].get(f'{src_lang}-{tgt_lang}'):\n",
    "            print(f\"Skipping {src_lang} to {tgt_lang}, already exists.\")\n",
    "            continue\n",
    "        out = translate(ds[src_lang]['text'], code_lang_map[src_lang], code_lang_map[tgt_lang])\n",
    "        results[src_lang][f'{src_lang}-{tgt_lang}'] = out\n",
    "\n",
    "        # One every translation pair, save to a file\n",
    "\n",
    "        try:\n",
    "            with open(\"translations_sarvam_flores.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "                print(f\"File saved as of {tgt_lang}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['eng_Latn'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a52c0c",
   "metadata": {},
   "source": [
    "## Evaluation with chrF++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db1543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-sarvam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
